/*
**
** PHiPAC Matrix-Matrix Code for the operation:
**    C = alpha*transpose(A)*transpose(B) + beta*C
**
** Automatically Generated by mm_gen ($Revision: 1.32 $) using the command:
**    ./../mm_gen/mm_gen -cb 16 1 2 -cb 1 32 32 -prec complex -file mdtmdt_acbc_md.c -routine_name mdtmdt_acbc_md -alpha c -beta c -opA T -opB T
**
** Run './../mm_gen/mm_gen -help' for help.
**
** Generated on: Thursday March 21 2002, 19:48:13 PST
** Created by: Jeff Bilmes <bilmes@cs.berkeley.edu>
**             http://www.icsi.berkeley.edu/~bilmes/phipac
**
**
** Usage:
**    mdtmdt_acbc_md(const int M, const int K, const int N, const complex *const A, const complex *const B, complex *const C, const int Astride, const int Bstride, const int Cstride, const complex alpha, const complex beta)
** where
**  transpose(A) is an MxK matrix
**  transpose(B) is an KxN matrix
**  C is an MxN matrix
**  Astride is the number of entries between the start of each row of A
**  Bstride is the number of entries between the start of each row of B
**  Cstride is the number of entries between the start of each row of C
**
**
** "Copyright (c) 1995 The Regents of the University of California.  All
** rights reserved."  Permission to use, copy, modify, and distribute
** this software and its documentation for any purpose, without fee, and
** without written agreement is hereby granted, provided that the above
** copyright notice and the following two paragraphs appear in all copies
** of this software.
**
** IN NO EVENT SHALL THE UNIVERSITY OF CALIFORNIA BE LIABLE TO ANY PARTY FOR
** DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT
** OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF THE UNIVERSITY OF
** CALIFORNIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
**
** THE UNIVERSITY OF CALIFORNIA SPECIFICALLY DISCLAIMS ANY WARRANTIES,
** INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY
** AND FITNESS FOR A PARTICULAR PURPOSE.  THE SOFTWARE PROVIDED HEREUNDER IS
** ON AN "AS IS" BASIS, AND THE UNIVERSITY OF CALIFORNIA HAS NO OBLIGATION TO
** PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.
**
*/

#include "container/complex.h"
#include "container/matrix/mdmd_cm.h"


BEGIN_BL_NAMESPACE


#define ZERO1x1(c00) \
{\
   c00 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE1x1(c00,C,Cstride) \
{\
   complex *_cp = C; \
   _cp[0] += alpha*c00; \
}

/* Fixed M,K,N = 1,1,1 fully-unrolled matrix matrix multiply. */
#define mul_tmd1x1tmd1x1_md1x1(c00,A,Astride,B0) \
{ \
    complex _a0; \
    complex _b0; \
   \
   \
   _a0 = A[0]; \
   A += Astride; \
   _b0 = B0[0]; \
   c00 += _b0*_a0; \
}


#define ZERO1x2(c00,c01) \
{\
   c00 = 0.0; c01 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE1x2(c00,c01,C,Cstride) \
{\
   complex *_cp = C; \
   _cp[0] += alpha*c00; _cp[1] += alpha*c01; \
}

/* Fixed M,K,N = 1,1,2 fully-unrolled matrix matrix multiply. */
#define mul_tmd1x1tmd1x2_md1x2(c00,c01,A,Astride,B0,B1) \
{ \
    complex _a0; \
    complex _b0,_b1; \
   \
   \
   _a0 = A[0]; \
   A += Astride; \
   _b0 = B0[0]; \
   c00 += _b0*_a0; \
   _b1 = B1[0]; \
   c01 += _b1*_a0; \
}


#define ZERO2x1(c00,c10) \
{\
   c00 = 0.0; \
   c10 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE2x1(c00,c10,C,Cstride) \
{\
   complex *_cp = C; \
   _cp[0] += alpha*c00; \
   _cp += Cstride; \
   _cp[0] += alpha*c10; \
}

/* Fixed M,K,N = 2,1,1 fully-unrolled matrix matrix multiply. */
#define mul_tmd2x1tmd1x1_md2x1(c00,c10,A,Astride,B0) \
{ \
    complex _a0,_a1; \
    complex _b0; \
   \
   \
   _a0 = A[0]; _a1 = A[1]; \
   A += Astride; \
   _b0 = B0[0]; \
   c00 += _b0*_a0; c10 += _b0*_a1; \
}


#define ZERO2x2(c00,c01,c10,c11) \
{\
   c00 = 0.0; c01 = 0.0; \
   c10 = 0.0; c11 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE2x2(c00,c01,c10,c11,C,Cstride) \
{\
   complex *_cp = C; \
   _cp[0] += alpha*c00; _cp[1] += alpha*c01; \
   _cp += Cstride; \
   _cp[0] += alpha*c10; _cp[1] += alpha*c11; \
}

/* Fixed M,K,N = 2,1,2 fully-unrolled matrix matrix multiply. */
#define mul_tmd2x1tmd1x2_md2x2(c00,c01,c10,c11,A,Astride,B0,B1) \
{ \
    complex _a0,_a1; \
    complex _b0,_b1; \
   \
   \
   _a0 = A[0]; _a1 = A[1]; \
   A += Astride; \
   _b0 = B0[0]; \
   c00 += _b0*_a0; c10 += _b0*_a1; \
   _b1 = B1[0]; \
   c01 += _b1*_a0; c11 += _b1*_a1; \
}


#define ZERO4x1(c00,c10,c20,c30) \
{\
   c00 = 0.0; \
   c10 = 0.0; \
   c20 = 0.0; \
   c30 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE4x1(c00,c10,c20,c30,C,Cstride) \
{\
   complex *_cp = C; \
   _cp[0] += alpha*c00; \
   _cp += Cstride; \
   _cp[0] += alpha*c10; \
   _cp += Cstride; \
   _cp[0] += alpha*c20; \
   _cp += Cstride; \
   _cp[0] += alpha*c30; \
}

/* Fixed M,K,N = 4,1,1 fully-unrolled matrix matrix multiply. */
#define mul_tmd4x1tmd1x1_md4x1(c00,c10,c20,c30,A,Astride,B0) \
{ \
    complex _a0,_a1,_a2,_a3; \
    complex _b0; \
   \
   \
   _a0 = A[0]; _a1 = A[1]; _a2 = A[2]; _a3 = A[3]; \
   A += Astride; \
   _b0 = B0[0]; \
   c00 += _b0*_a0; c10 += _b0*_a1; c20 += _b0*_a2; c30 += _b0*_a3; \
}


#define ZERO4x2(c00,c01,c10,c11,c20,c21,c30,c31) \
{\
   c00 = 0.0; c01 = 0.0; \
   c10 = 0.0; c11 = 0.0; \
   c20 = 0.0; c21 = 0.0; \
   c30 = 0.0; c31 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE4x2(c00,c01,c10,c11,c20,c21,c30,c31,C,Cstride) \
{\
   complex *_cp = C; \
   _cp[0] += alpha*c00; _cp[1] += alpha*c01; \
   _cp += Cstride; \
   _cp[0] += alpha*c10; _cp[1] += alpha*c11; \
   _cp += Cstride; \
   _cp[0] += alpha*c20; _cp[1] += alpha*c21; \
   _cp += Cstride; \
   _cp[0] += alpha*c30; _cp[1] += alpha*c31; \
}

/* Fixed M,K,N = 4,1,2 fully-unrolled matrix matrix multiply. */
#define mul_tmd4x1tmd1x2_md4x2(c00,c01,c10,c11,c20,c21,c30,c31,A,Astride,B0,B1) \
{ \
    complex _a0,_a1,_a2,_a3; \
    complex _b0,_b1; \
   \
   \
   _a0 = A[0]; _a1 = A[1]; _a2 = A[2]; _a3 = A[3]; \
   A += Astride; \
   _b0 = B0[0]; \
   c00 += _b0*_a0; c10 += _b0*_a1; c20 += _b0*_a2; c30 += _b0*_a3; \
   _b1 = B1[0]; \
   c01 += _b1*_a0; c11 += _b1*_a1; c21 += _b1*_a2; c31 += _b1*_a3; \
}


#define ZERO8x1(c00,c10,c20,c30,c40,c50,c60,c70) \
{\
   c00 = 0.0; \
   c10 = 0.0; \
   c20 = 0.0; \
   c30 = 0.0; \
   c40 = 0.0; \
   c50 = 0.0; \
   c60 = 0.0; \
   c70 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE8x1(c00,c10,c20,c30,c40,c50,c60,c70,C,Cstride) \
{\
   complex *_cp = C; \
   _cp[0] += alpha*c00; \
   _cp += Cstride; \
   _cp[0] += alpha*c10; \
   _cp += Cstride; \
   _cp[0] += alpha*c20; \
   _cp += Cstride; \
   _cp[0] += alpha*c30; \
   _cp += Cstride; \
   _cp[0] += alpha*c40; \
   _cp += Cstride; \
   _cp[0] += alpha*c50; \
   _cp += Cstride; \
   _cp[0] += alpha*c60; \
   _cp += Cstride; \
   _cp[0] += alpha*c70; \
}

/* Fixed M,K,N = 8,1,1 fully-unrolled matrix matrix multiply. */
#define mul_tmd8x1tmd1x1_md8x1(c00,c10,c20,c30,c40,c50,c60,c70,A,Astride,B0) \
{ \
    complex _a0,_a1,_a2,_a3,_a4,_a5,_a6,_a7; \
    complex _b0; \
   \
   \
   _a0 = A[0]; _a1 = A[1]; _a2 = A[2]; _a3 = A[3]; _a4 = A[4]; _a5 = A[5]; _a6 = A[6]; _a7 = A[7]; \
   A += Astride; \
   _b0 = B0[0]; \
   c00 += _b0*_a0; c10 += _b0*_a1; c20 += _b0*_a2; c30 += _b0*_a3; c40 += _b0*_a4; c50 += _b0*_a5; c60 += _b0*_a6; c70 += _b0*_a7; \
}


#define ZERO8x2(c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71) \
{\
   c00 = 0.0; c01 = 0.0; \
   c10 = 0.0; c11 = 0.0; \
   c20 = 0.0; c21 = 0.0; \
   c30 = 0.0; c31 = 0.0; \
   c40 = 0.0; c41 = 0.0; \
   c50 = 0.0; c51 = 0.0; \
   c60 = 0.0; c61 = 0.0; \
   c70 = 0.0; c71 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE8x2(c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71,C,Cstride) \
{\
   complex *_cp = C; \
   _cp[0] += alpha*c00; _cp[1] += alpha*c01; \
   _cp += Cstride; \
   _cp[0] += alpha*c10; _cp[1] += alpha*c11; \
   _cp += Cstride; \
   _cp[0] += alpha*c20; _cp[1] += alpha*c21; \
   _cp += Cstride; \
   _cp[0] += alpha*c30; _cp[1] += alpha*c31; \
   _cp += Cstride; \
   _cp[0] += alpha*c40; _cp[1] += alpha*c41; \
   _cp += Cstride; \
   _cp[0] += alpha*c50; _cp[1] += alpha*c51; \
   _cp += Cstride; \
   _cp[0] += alpha*c60; _cp[1] += alpha*c61; \
   _cp += Cstride; \
   _cp[0] += alpha*c70; _cp[1] += alpha*c71; \
}

/* Fixed M,K,N = 8,1,2 fully-unrolled matrix matrix multiply. */
#define mul_tmd8x1tmd1x2_md8x2(c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71,A,Astride,B0,B1) \
{ \
    complex _a0,_a1,_a2,_a3,_a4,_a5,_a6,_a7; \
    complex _b0,_b1; \
   \
   \
   _a0 = A[0]; _a1 = A[1]; _a2 = A[2]; _a3 = A[3]; _a4 = A[4]; _a5 = A[5]; _a6 = A[6]; _a7 = A[7]; \
   A += Astride; \
   _b0 = B0[0]; \
   c00 += _b0*_a0; c10 += _b0*_a1; c20 += _b0*_a2; c30 += _b0*_a3; c40 += _b0*_a4; c50 += _b0*_a5; c60 += _b0*_a6; c70 += _b0*_a7; \
   _b1 = B1[0]; \
   c01 += _b1*_a0; c11 += _b1*_a1; c21 += _b1*_a2; c31 += _b1*_a3; c41 += _b1*_a4; c51 += _b1*_a5; c61 += _b1*_a6; c71 += _b1*_a7; \
}


#define ZERO16x1(c00,c10,c20,c30,c40,c50,c60,c70,c80,c90,c100,c110,c120,c130,c140,c150) \
{\
   c00 = 0.0; \
   c10 = 0.0; \
   c20 = 0.0; \
   c30 = 0.0; \
   c40 = 0.0; \
   c50 = 0.0; \
   c60 = 0.0; \
   c70 = 0.0; \
   c80 = 0.0; \
   c90 = 0.0; \
   c100 = 0.0; \
   c110 = 0.0; \
   c120 = 0.0; \
   c130 = 0.0; \
   c140 = 0.0; \
   c150 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE16x1(c00,c10,c20,c30,c40,c50,c60,c70,c80,c90,c100,c110,c120,c130,c140,c150,C,Cstride) \
{\
   complex *_cp = C; \
   _cp[0] += alpha*c00; \
   _cp += Cstride; \
   _cp[0] += alpha*c10; \
   _cp += Cstride; \
   _cp[0] += alpha*c20; \
   _cp += Cstride; \
   _cp[0] += alpha*c30; \
   _cp += Cstride; \
   _cp[0] += alpha*c40; \
   _cp += Cstride; \
   _cp[0] += alpha*c50; \
   _cp += Cstride; \
   _cp[0] += alpha*c60; \
   _cp += Cstride; \
   _cp[0] += alpha*c70; \
   _cp += Cstride; \
   _cp[0] += alpha*c80; \
   _cp += Cstride; \
   _cp[0] += alpha*c90; \
   _cp += Cstride; \
   _cp[0] += alpha*c100; \
   _cp += Cstride; \
   _cp[0] += alpha*c110; \
   _cp += Cstride; \
   _cp[0] += alpha*c120; \
   _cp += Cstride; \
   _cp[0] += alpha*c130; \
   _cp += Cstride; \
   _cp[0] += alpha*c140; \
   _cp += Cstride; \
   _cp[0] += alpha*c150; \
}

/* Fixed M,K,N = 16,1,1 fully-unrolled matrix matrix multiply. */
#define mul_tmd16x1tmd1x1_md16x1(c00,c10,c20,c30,c40,c50,c60,c70,c80,c90,c100,c110,c120,c130,c140,c150,A,Astride,B0) \
{ \
    complex _a0,_a1,_a2,_a3,_a4,_a5,_a6,_a7,_a8,_a9,_a10,_a11,_a12,_a13,_a14,_a15; \
    complex _b0; \
   \
   \
   _a0 = A[0]; _a1 = A[1]; _a2 = A[2]; _a3 = A[3]; _a4 = A[4]; _a5 = A[5]; _a6 = A[6]; _a7 = A[7]; _a8 = A[8]; _a9 = A[9]; _a10 = A[10]; _a11 = A[11]; _a12 = A[12]; _a13 = A[13]; _a14 = A[14]; _a15 = A[15]; \
   A += Astride; \
   _b0 = B0[0]; \
   c00 += _b0*_a0; c10 += _b0*_a1; c20 += _b0*_a2; c30 += _b0*_a3; c40 += _b0*_a4; c50 += _b0*_a5; c60 += _b0*_a6; c70 += _b0*_a7; c80 += _b0*_a8; c90 += _b0*_a9; c100 += _b0*_a10; c110 += _b0*_a11; c120 += _b0*_a12; c130 += _b0*_a13; c140 += _b0*_a14; c150 += _b0*_a15; \
}


#define ZERO16x2(c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71,c80,c81,c90,c91,c100,c101,c110,c111,c120,c121,c130,c131,c140,c141,c150,c151) \
{\
   c00 = 0.0; c01 = 0.0; \
   c10 = 0.0; c11 = 0.0; \
   c20 = 0.0; c21 = 0.0; \
   c30 = 0.0; c31 = 0.0; \
   c40 = 0.0; c41 = 0.0; \
   c50 = 0.0; c51 = 0.0; \
   c60 = 0.0; c61 = 0.0; \
   c70 = 0.0; c71 = 0.0; \
   c80 = 0.0; c81 = 0.0; \
   c90 = 0.0; c91 = 0.0; \
   c100 = 0.0; c101 = 0.0; \
   c110 = 0.0; c111 = 0.0; \
   c120 = 0.0; c121 = 0.0; \
   c130 = 0.0; c131 = 0.0; \
   c140 = 0.0; c141 = 0.0; \
   c150 = 0.0; c151 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE16x2(c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71,c80,c81,c90,c91,c100,c101,c110,c111,c120,c121,c130,c131,c140,c141,c150,c151,C,Cstride) \
{\
   complex *_cp = C; \
   _cp[0] += alpha*c00; _cp[1] += alpha*c01; \
   _cp += Cstride; \
   _cp[0] += alpha*c10; _cp[1] += alpha*c11; \
   _cp += Cstride; \
   _cp[0] += alpha*c20; _cp[1] += alpha*c21; \
   _cp += Cstride; \
   _cp[0] += alpha*c30; _cp[1] += alpha*c31; \
   _cp += Cstride; \
   _cp[0] += alpha*c40; _cp[1] += alpha*c41; \
   _cp += Cstride; \
   _cp[0] += alpha*c50; _cp[1] += alpha*c51; \
   _cp += Cstride; \
   _cp[0] += alpha*c60; _cp[1] += alpha*c61; \
   _cp += Cstride; \
   _cp[0] += alpha*c70; _cp[1] += alpha*c71; \
   _cp += Cstride; \
   _cp[0] += alpha*c80; _cp[1] += alpha*c81; \
   _cp += Cstride; \
   _cp[0] += alpha*c90; _cp[1] += alpha*c91; \
   _cp += Cstride; \
   _cp[0] += alpha*c100; _cp[1] += alpha*c101; \
   _cp += Cstride; \
   _cp[0] += alpha*c110; _cp[1] += alpha*c111; \
   _cp += Cstride; \
   _cp[0] += alpha*c120; _cp[1] += alpha*c121; \
   _cp += Cstride; \
   _cp[0] += alpha*c130; _cp[1] += alpha*c131; \
   _cp += Cstride; \
   _cp[0] += alpha*c140; _cp[1] += alpha*c141; \
   _cp += Cstride; \
   _cp[0] += alpha*c150; _cp[1] += alpha*c151; \
}

/* Fixed M,K,N = 16,1,2 fully-unrolled matrix matrix multiply. */
#define mul_tmd16x1tmd1x2_md16x2(c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71,c80,c81,c90,c91,c100,c101,c110,c111,c120,c121,c130,c131,c140,c141,c150,c151,A,Astride,B0,B1) \
{ \
    complex _a0,_a1,_a2,_a3,_a4,_a5,_a6,_a7,_a8,_a9,_a10,_a11,_a12,_a13,_a14,_a15; \
    complex _b0,_b1; \
   \
   \
   _a0 = A[0]; _a1 = A[1]; _a2 = A[2]; _a3 = A[3]; _a4 = A[4]; _a5 = A[5]; _a6 = A[6]; _a7 = A[7]; _a8 = A[8]; _a9 = A[9]; _a10 = A[10]; _a11 = A[11]; _a12 = A[12]; _a13 = A[13]; _a14 = A[14]; _a15 = A[15]; \
   A += Astride; \
   _b0 = B0[0]; \
   c00 += _b0*_a0; c10 += _b0*_a1; c20 += _b0*_a2; c30 += _b0*_a3; c40 += _b0*_a4; c50 += _b0*_a5; c60 += _b0*_a6; c70 += _b0*_a7; c80 += _b0*_a8; c90 += _b0*_a9; c100 += _b0*_a10; c110 += _b0*_a11; c120 += _b0*_a12; c130 += _b0*_a13; c140 += _b0*_a14; c150 += _b0*_a15; \
   _b1 = B1[0]; \
   c01 += _b1*_a0; c11 += _b1*_a1; c21 += _b1*_a2; c31 += _b1*_a3; c41 += _b1*_a4; c51 += _b1*_a5; c61 += _b1*_a6; c71 += _b1*_a7; c81 += _b1*_a8; c91 += _b1*_a9; c101 += _b1*_a10; c111 += _b1*_a11; c121 += _b1*_a12; c131 += _b1*_a13; c141 += _b1*_a14; c151 += _b1*_a15; \
}


/* Fixed M,N = 16,64, Arbitrary K L0-blocked matrix matrix multiply. */
static void
mdtmdt_acbc_md_l1_arb_k(int K, const complex *const A, const complex *const B, complex *const C, const int Astride, const int Bstride, const int Cstride, const complex alpha)
{
   const complex *a0,*b0;
   complex *c0;
   const complex *ap0;
   const complex *bp0_0,*bp0_1;
   complex *cp0;
   const int B_sbs_stride = Bstride*2;
   const int C_sbs_stride = Cstride*16;
   const int k_marg_el = K & 0;
   const int k_norm = (K - k_marg_el)*Astride;
   complex *const c0_endp = C+16*Cstride;
    complex c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71,c80,c81,c90,c91,c100,c101,c110,c111,c120,c121,c130,c131,c140,c141,c150,c151;
   for (c0=C,a0=A; c0!= c0_endp; c0+=C_sbs_stride,a0+=16) {
      const complex* const ap0_endp = a0 + k_norm;
      complex* const cp0_endp = c0 + 64;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=B_sbs_stride,cp0+=2) {
         ap0=a0;
         bp0_0 = b0;
         bp0_1 = bp0_0 + Bstride;
         ZERO16x2(c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71,c80,c81,c90,c91,c100,c101,c110,c111,c120,c121,c130,c131,c140,c141,c150,c151);
         for (; ap0!=ap0_endp; bp0_0+=1,bp0_1+=1) {
            mul_tmd16x1tmd1x2_md16x2(c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71,c80,c81,c90,c91,c100,c101,c110,c111,c120,c121,c130,c131,c140,c141,c150,c151,ap0,Astride,bp0_0,bp0_1);
         }
         SCALE_ACCUMULATE_STORE16x2(c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71,c80,c81,c90,c91,c100,c101,c110,c111,c120,c121,c130,c131,c140,c141,c150,c151,cp0,Cstride);
      }
   }
}

/* Arbitrary M,K,N L0-blocked matrix matrix multiply. */
static void
mdtmdt_acbc_md_l1_arb_all(const int M, const int K, const int N, const complex *const A, const complex *const B, complex *const C, const int Astride, const int Bstride, const int Cstride, const complex alpha)
{
   const complex *a0,*b0;
   complex *c0;
   const complex *ap0;
   const complex *bp0_0,*bp0_1;
   complex *cp0;
   const int B_sbs_stride = Bstride*2;
   const int C_sbs_stride = Cstride*16;
   const int k_marg_el = K & 0;
   const int k_norm = (K - k_marg_el)*Astride;
   const int m_marg_el = M & 15;
   const int m_norm = M - m_marg_el;
   const int n_marg_el = N & 1;
   const int n_norm = N - n_marg_el;
   complex *const c0_endp = C+m_norm*Cstride;
    complex c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71,c80,c81,c90,c91,c100,c101,c110,c111,c120,c121,c130,c131,c140,c141,c150,c151;
   for (c0=C,a0=A; c0!= c0_endp; c0+=C_sbs_stride,a0+=16) {
      const complex* const ap0_endp = a0 + k_norm;
      complex* const cp0_endp = c0 + n_norm;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=B_sbs_stride,cp0+=2) {
         ap0=a0;
         bp0_0 = b0;
         bp0_1 = bp0_0 + Bstride;
         ZERO16x2(c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71,c80,c81,c90,c91,c100,c101,c110,c111,c120,c121,c130,c131,c140,c141,c150,c151);
         for (; ap0!=ap0_endp; bp0_0+=1,bp0_1+=1) {
            mul_tmd16x1tmd1x2_md16x2(c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71,c80,c81,c90,c91,c100,c101,c110,c111,c120,c121,c130,c131,c140,c141,c150,c151,ap0,Astride,bp0_0,bp0_1);
         }
         SCALE_ACCUMULATE_STORE16x2(c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71,c80,c81,c90,c91,c100,c101,c110,c111,c120,c121,c130,c131,c140,c141,c150,c151,cp0,Cstride);
      }
   }
   for (c0=C,a0=A; c0!= c0_endp; c0+=C_sbs_stride,a0+=16) {
      const complex* const ap0_endp = a0 + k_norm;
      b0 = B+n_norm*Bstride;
      cp0 = c0+n_norm;
      if (n_marg_el & 0x1) {
         ap0=a0;
         bp0_0 = b0;
         ZERO16x1(c00,c10,c20,c30,c40,c50,c60,c70,c80,c90,c100,c110,c120,c130,c140,c150);
         for (; ap0!=ap0_endp; bp0_0+=1) {
            mul_tmd16x1tmd1x1_md16x1(c00,c10,c20,c30,c40,c50,c60,c70,c80,c90,c100,c110,c120,c130,c140,c150,ap0,Astride,bp0_0);
         }
         SCALE_ACCUMULATE_STORE16x1(c00,c10,c20,c30,c40,c50,c60,c70,c80,c90,c100,c110,c120,c130,c140,c150,cp0,Cstride);
      }
   }
   if (m_marg_el & 0x8) {
      const complex* const ap0_endp = a0 + k_norm;
      complex* const cp0_endp = c0 + n_norm;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=B_sbs_stride,cp0+=2) {
         ap0=a0;
         bp0_0 = b0;
         bp0_1 = bp0_0 + Bstride;
         ZERO8x2(c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71);
         for (; ap0!=ap0_endp; bp0_0+=1,bp0_1+=1) {
            mul_tmd8x1tmd1x2_md8x2(c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71,ap0,Astride,bp0_0,bp0_1);
         }
         SCALE_ACCUMULATE_STORE8x2(c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71,cp0,Cstride);
      }
      if (n_marg_el & 0x1) {
         ap0=a0;
         bp0_0 = b0;
         ZERO8x1(c00,c10,c20,c30,c40,c50,c60,c70);
         for (; ap0!=ap0_endp; bp0_0+=1) {
            mul_tmd8x1tmd1x1_md8x1(c00,c10,c20,c30,c40,c50,c60,c70,ap0,Astride,bp0_0);
         }
         SCALE_ACCUMULATE_STORE8x1(c00,c10,c20,c30,c40,c50,c60,c70,cp0,Cstride);
      }
      c0 += Cstride*8;
      a0 += 8;
   }
   if (m_marg_el & 0x4) {
      const complex* const ap0_endp = a0 + k_norm;
      complex* const cp0_endp = c0 + n_norm;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=B_sbs_stride,cp0+=2) {
         ap0=a0;
         bp0_0 = b0;
         bp0_1 = bp0_0 + Bstride;
         ZERO4x2(c00,c01,c10,c11,c20,c21,c30,c31);
         for (; ap0!=ap0_endp; bp0_0+=1,bp0_1+=1) {
            mul_tmd4x1tmd1x2_md4x2(c00,c01,c10,c11,c20,c21,c30,c31,ap0,Astride,bp0_0,bp0_1);
         }
         SCALE_ACCUMULATE_STORE4x2(c00,c01,c10,c11,c20,c21,c30,c31,cp0,Cstride);
      }
      if (n_marg_el & 0x1) {
         ap0=a0;
         bp0_0 = b0;
         ZERO4x1(c00,c10,c20,c30);
         for (; ap0!=ap0_endp; bp0_0+=1) {
            mul_tmd4x1tmd1x1_md4x1(c00,c10,c20,c30,ap0,Astride,bp0_0);
         }
         SCALE_ACCUMULATE_STORE4x1(c00,c10,c20,c30,cp0,Cstride);
      }
      c0 += Cstride*4;
      a0 += 4;
   }
   if (m_marg_el & 0x2) {
      const complex* const ap0_endp = a0 + k_norm;
      complex* const cp0_endp = c0 + n_norm;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=B_sbs_stride,cp0+=2) {
         ap0=a0;
         bp0_0 = b0;
         bp0_1 = bp0_0 + Bstride;
         ZERO2x2(c00,c01,c10,c11);
         for (; ap0!=ap0_endp; bp0_0+=1,bp0_1+=1) {
            mul_tmd2x1tmd1x2_md2x2(c00,c01,c10,c11,ap0,Astride,bp0_0,bp0_1);
         }
         SCALE_ACCUMULATE_STORE2x2(c00,c01,c10,c11,cp0,Cstride);
      }
      if (n_marg_el & 0x1) {
         ap0=a0;
         bp0_0 = b0;
         ZERO2x1(c00,c10);
         for (; ap0!=ap0_endp; bp0_0+=1) {
            mul_tmd2x1tmd1x1_md2x1(c00,c10,ap0,Astride,bp0_0);
         }
         SCALE_ACCUMULATE_STORE2x1(c00,c10,cp0,Cstride);
      }
      c0 += Cstride*2;
      a0 += 2;
   }
   if (m_marg_el & 0x1) {
      const complex* const ap0_endp = a0 + k_norm;
      complex* const cp0_endp = c0 + n_norm;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=B_sbs_stride,cp0+=2) {
         ap0=a0;
         bp0_0 = b0;
         bp0_1 = bp0_0 + Bstride;
         ZERO1x2(c00,c01);
         for (; ap0!=ap0_endp; bp0_0+=1,bp0_1+=1) {
            mul_tmd1x1tmd1x2_md1x2(c00,c01,ap0,Astride,bp0_0,bp0_1);
         }
         SCALE_ACCUMULATE_STORE1x2(c00,c01,cp0,Cstride);
      }
      if (n_marg_el & 0x1) {
         ap0=a0;
         bp0_0 = b0;
         ZERO1x1(c00);
         for (; ap0!=ap0_endp; bp0_0+=1) {
            mul_tmd1x1tmd1x1_md1x1(c00,ap0,Astride,bp0_0);
         }
         SCALE_ACCUMULATE_STORE1x1(c00,cp0,Cstride);
      }
   }
}

/* Fixed M,K,N = 16,32,64 L0-blocked matrix matrix multiply. */
static void
mdtmdt_acbc_md_l1(const complex *const A, const complex *const B, complex *const C, const int Astride, const int Bstride, const int Cstride, const complex alpha)
{
   const complex *a0,*b0;
   complex *c0;
   const complex *ap0;
   const complex *bp0_0,*bp0_1;
   complex *cp0;
   const int B_sbs_stride = Bstride*2;
   const int C_sbs_stride = Cstride*16;
   const int k_norm = 32*Astride;
   complex *const c0_endp = C+16*Cstride;
    complex c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71,c80,c81,c90,c91,c100,c101,c110,c111,c120,c121,c130,c131,c140,c141,c150,c151;
   for (c0=C,a0=A; c0!= c0_endp; c0+=C_sbs_stride,a0+=16) {
      const complex* const ap0_endp = a0 + k_norm;
      complex* const cp0_endp = c0 + 64;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=B_sbs_stride,cp0+=2) {
         ap0=a0;
         bp0_0 = b0;
         bp0_1 = bp0_0 + Bstride;
         ZERO16x2(c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71,c80,c81,c90,c91,c100,c101,c110,c111,c120,c121,c130,c131,c140,c141,c150,c151);
         for (; ap0!=ap0_endp; bp0_0+=1,bp0_1+=1) {
            mul_tmd16x1tmd1x2_md16x2(c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71,c80,c81,c90,c91,c100,c101,c110,c111,c120,c121,c130,c131,c140,c141,c150,c151,ap0,Astride,bp0_0,bp0_1);
         }
         SCALE_ACCUMULATE_STORE16x2(c00,c01,c10,c11,c20,c21,c30,c31,c40,c41,c50,c51,c60,c61,c70,c71,c80,c81,c90,c91,c100,c101,c110,c111,c120,c121,c130,c131,c140,c141,c150,c151,cp0,Cstride);
      }
   }
}

void
mdtmdt_acbc_md_cm(const int M, const int K, const int N, const complex *const A, const complex *const B, complex *const C, const int Astride, const int Bstride, const int Cstride, const complex alpha, const complex beta)
{
   /* Code for L1-blocked routine. */
   int m2,k2,n2;
   const complex *a2,*b2;
   complex *c2;
   const complex *ap2,*bp2;
   complex *cp2;
   {
      complex *cprb,*cpre,*cp,*cpe;
      cpre = C + M*Cstride;
      for (cprb = C; cprb != cpre; cprb += Cstride) {
         cpe = cprb + N;
         for (cp = cprb; cp != cpe; cp++) {
            *cp *= (beta);
         }
      }
   }
   if (alpha == 0.0) {
      return;
   }
   if (M < 17 && K < 33 && N < 65) {
      mdtmdt_acbc_md_l1_arb_all(M,K,N,A,B,C,Astride,Bstride,Cstride,alpha);
      return;
   }
   for (m2=0; m2<=M-16; m2+=16) {
      c2 = C + m2*Cstride;
      a2 = A + m2;
      for (n2=0,b2=B,cp2=c2; n2<=N-64; n2+=64,b2+=64*Bstride,cp2+=64) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-32; k2+=32,bp2+=32,ap2+=32*Astride) {
            mdtmdt_acbc_md_l1(ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
         if (k2 < K) {
            mdtmdt_acbc_md_l1_arb_k(K-k2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
      }
      if (n2 < N) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-32; k2+=32,bp2+=32,ap2+=32*Astride) {
            mdtmdt_acbc_md_l1_arb_all(16,32,N-n2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
         if (k2 < K) {
            mdtmdt_acbc_md_l1_arb_all(16,K-k2,N-n2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
      }
   }
   if (m2 < M) {
      c2 = C + m2*Cstride;
      a2 = A + m2;
      for (n2=0,b2=B,cp2=c2; n2<=N-64; n2+=64,b2+=64*Bstride,cp2+=64) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-32; k2+=32,bp2+=32,ap2+=32*Astride) {
            mdtmdt_acbc_md_l1_arb_all(M-m2,32,64,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
         if (k2 < K) {
            mdtmdt_acbc_md_l1_arb_all(M-m2,K-k2,64,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
      }
      if (n2 < N) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-32; k2+=32,bp2+=32,ap2+=32*Astride) {
            mdtmdt_acbc_md_l1_arb_all(M-m2,32,N-n2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
         if (k2 < K) {
            mdtmdt_acbc_md_l1_arb_all(M-m2,K-k2,N-n2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
      }
   }
}

END_BL_NAMESPACE

