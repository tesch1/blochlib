/*
**
** PHiPAC Matrix-Matrix Code for the operation:
**    C = alpha*transpose(A)*B + beta*C
**
** Automatically Generated by mm_gen ($Revision: 1.32 $) using the command:
**    ./../mm_gen/mm_gen -cb 4 3 1 -cb 20 26 20 -cb 442 147 1180 -prec T -file mdtmd_acbc_md.c -routine_name mdtmd_acbc_md -alpha c -beta c -opA T
**
** Run './../mm_gen/mm_gen -help' for help.
**
** Generated on: Thursday March 21 2002, 09:10:47 PST
** Created by: Jeff Bilmes <bilmes@cs.berkeley.edu>
**             http://www.icsi.berkeley.edu/~bilmes/phipac
**
**
** Usage:
**    mdtmd_acbc_md(const int M, const int K, const int N, const T *const A, const T *const B, T *const C, const int Astride, const int Bstride, const int Cstride, const T alpha, const T beta)
** where
**  transpose(A) is an MxK matrix
**  B is an KxN matrix
**  C is an MxN matrix
**  Astride is the number of entries between the start of each row of A
**  Bstride is the number of entries between the start of each row of B
**  Cstride is the number of entries between the start of each row of C
**
**
** "Copyright (c) 1995 The Regents of the University of California.  All
** rights reserved."  Permission to use, copy, modify, and distribute
** this software and its documentation for any purpose, without fee, and
** without written agreement is hereby granted, provided that the above
** copyright notice and the following two paragraphs appear in all copies
** of this software.
**
** IN NO EVENT SHALL THE UNIVERSITY OF CALIFORNIA BE LIABLE TO ANY PARTY FOR
** DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT
** OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF THE UNIVERSITY OF
** CALIFORNIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
**
** THE UNIVERSITY OF CALIFORNIA SPECIFICALLY DISCLAIMS ANY WARRANTIES,
** INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY
** AND FITNESS FOR A PARTICULAR PURPOSE.  THE SOFTWARE PROVIDED HEREUNDER IS
** ON AN "AS IS" BASIS, AND THE UNIVERSITY OF CALIFORNIA HAS NO OBLIGATION TO
** PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.
**
*/

#ifndef _BLPHIP_mdtmd_acbc_md_h_
#define _BLPHIP_mdtmd_acbc_md_h_ 1


BEGIN_BL_NAMESPACE



#define ZERO1x1(c00) \
{\
   c00 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE1x1(c00,C,Cstride) \
{\
   T *_cp = C; \
   _cp[0] += alpha*c00; \
}

/* Fixed M,K,N = 1,1,1 fully-unrolled matrix matrix multiply. */
#define mul_tmd1x1md1x1_md1x1(c00,A,Astride,B,Bstride) \
{ \
    T _b0; \
    T _a0; \
   \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   A += Astride; \
}


/* Fixed M,K,N = 1,2,1 fully-unrolled matrix matrix multiply. */
#define mul_tmd1x2md2x1_md1x1(c00,A,Astride,B,Bstride) \
{ \
    T _b0; \
    T _a0; \
   \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   A += Astride; \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   A += Astride; \
}


/* Fixed M,K,N = 1,3,1 fully-unrolled matrix matrix multiply. */
#define mul_tmd1x3md3x1_md1x1(c00,A,Astride,B,Bstride) \
{ \
    T _b0; \
    T _a0; \
   \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   A += Astride; \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   A += Astride; \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   A += Astride; \
}


#define ZERO2x1(c00,c10) \
{\
   c00 = 0.0; \
   c10 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE2x1(c00,c10,C,Cstride) \
{\
   T *_cp = C; \
   _cp[0] += alpha*c00; \
   _cp += Cstride; \
   _cp[0] += alpha*c10; \
}

/* Fixed M,K,N = 2,1,1 fully-unrolled matrix matrix multiply. */
#define mul_tmd2x1md1x1_md2x1(c00,c10,A,Astride,B,Bstride) \
{ \
    T _b0; \
    T _a0,_a1; \
   \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   _a1 = A[1]; \
   c10 += _a1*_b0; \
   A += Astride; \
}


/* Fixed M,K,N = 2,2,1 fully-unrolled matrix matrix multiply. */
#define mul_tmd2x2md2x1_md2x1(c00,c10,A,Astride,B,Bstride) \
{ \
    T _b0; \
    T _a0,_a1; \
   \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   _a1 = A[1]; \
   c10 += _a1*_b0; \
   A += Astride; \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   _a1 = A[1]; \
   c10 += _a1*_b0; \
   A += Astride; \
}


/* Fixed M,K,N = 2,3,1 fully-unrolled matrix matrix multiply. */
#define mul_tmd2x3md3x1_md2x1(c00,c10,A,Astride,B,Bstride) \
{ \
    T _b0; \
    T _a0,_a1; \
   \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   _a1 = A[1]; \
   c10 += _a1*_b0; \
   A += Astride; \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   _a1 = A[1]; \
   c10 += _a1*_b0; \
   A += Astride; \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   _a1 = A[1]; \
   c10 += _a1*_b0; \
   A += Astride; \
}


#define ZERO4x1(c00,c10,c20,c30) \
{\
   c00 = 0.0; \
   c10 = 0.0; \
   c20 = 0.0; \
   c30 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE4x1(c00,c10,c20,c30,C,Cstride) \
{\
   T *_cp = C; \
   _cp[0] += alpha*c00; \
   _cp += Cstride; \
   _cp[0] += alpha*c10; \
   _cp += Cstride; \
   _cp[0] += alpha*c20; \
   _cp += Cstride; \
   _cp[0] += alpha*c30; \
}

/* Fixed M,K,N = 4,1,1 fully-unrolled matrix matrix multiply. */
#define mul_tmd4x1md1x1_md4x1(c00,c10,c20,c30,A,Astride,B,Bstride) \
{ \
    T _b0; \
    T _a0,_a1,_a2,_a3; \
   \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   _a1 = A[1]; \
   c10 += _a1*_b0; \
   _a2 = A[2]; \
   c20 += _a2*_b0; \
   _a3 = A[3]; \
   c30 += _a3*_b0; \
   A += Astride; \
}


/* Fixed M,K,N = 4,2,1 fully-unrolled matrix matrix multiply. */
#define mul_tmd4x2md2x1_md4x1(c00,c10,c20,c30,A,Astride,B,Bstride) \
{ \
    T _b0; \
    T _a0,_a1,_a2,_a3; \
   \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   _a1 = A[1]; \
   c10 += _a1*_b0; \
   _a2 = A[2]; \
   c20 += _a2*_b0; \
   _a3 = A[3]; \
   c30 += _a3*_b0; \
   A += Astride; \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   _a1 = A[1]; \
   c10 += _a1*_b0; \
   _a2 = A[2]; \
   c20 += _a2*_b0; \
   _a3 = A[3]; \
   c30 += _a3*_b0; \
   A += Astride; \
}


/* Fixed M,K,N = 4,3,1 fully-unrolled matrix matrix multiply. */
#define mul_tmd4x3md3x1_md4x1(c00,c10,c20,c30,A,Astride,B,Bstride) \
{ \
    T _b0; \
    T _a0,_a1,_a2,_a3; \
   \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   _a1 = A[1]; \
   c10 += _a1*_b0; \
   _a2 = A[2]; \
   c20 += _a2*_b0; \
   _a3 = A[3]; \
   c30 += _a3*_b0; \
   A += Astride; \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   _a1 = A[1]; \
   c10 += _a1*_b0; \
   _a2 = A[2]; \
   c20 += _a2*_b0; \
   _a3 = A[3]; \
   c30 += _a3*_b0; \
   A += Astride; \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a0 = A[0]; \
   c00 += _a0*_b0; \
   _a1 = A[1]; \
   c10 += _a1*_b0; \
   _a2 = A[2]; \
   c20 += _a2*_b0; \
   _a3 = A[3]; \
   c30 += _a3*_b0; \
   A += Astride; \
}


/* Fixed M,N = 80,20, Arbitrary K L0-blocked matrix matrix multiply. */
template<class T>
static void
mdtmd_acbc_md_l1_arb_k(int K, const T *const A, const T *const B, T *const C, const int Astride, const int Bstride, const int Cstride, const T alpha)
{
   const T *a0,*b0;
   T *c0;
   const T *ap0;
   const T *bp0;
   T *cp0;
   const int C_sbs_stride = Cstride*4;
   const int k_marg_el = K % 3;
   const int k_norm = (K - k_marg_el)*Astride;
   T *const c0_endp = C+80*Cstride;
    T c00,c10,c20,c30;
   for (c0=C,a0=A; c0!= c0_endp; c0+=C_sbs_stride,a0+=4) {
      const T* const ap0_endp = a0 + k_norm;
      T* const cp0_endp = c0 + 20;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=1,cp0+=1) {
         ap0=a0;
         bp0=b0;
         ZERO4x1(c00,c10,c20,c30);
         for (; ap0!=ap0_endp; ) {
            mul_tmd4x3md3x1_md4x1(c00,c10,c20,c30,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x2) {
            mul_tmd4x2md2x1_md4x1(c00,c10,c20,c30,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmd4x1md1x1_md4x1(c00,c10,c20,c30,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE4x1(c00,c10,c20,c30,cp0,Cstride);
      }
   }
}

/* Arbitrary M,K,N L0-blocked matrix matrix multiply. */
template<class T>
static void
mdtmd_acbc_md_l1_arb_all(const int M, const int K, const int N, const T *const A, const T *const B, T *const C, const int Astride, const int Bstride, const int Cstride, const T alpha)
{
   const T *a0,*b0;
   T *c0;
   const T *ap0;
   const T *bp0;
   T *cp0;
   const int C_sbs_stride = Cstride*4;
   const int k_marg_el = K % 3;
   const int k_norm = (K - k_marg_el)*Astride;
   const int m_marg_el = M & 3;
   const int m_norm = M - m_marg_el;
   const int n_marg_el = N & 0;
   const int n_norm = N - n_marg_el;
   T *const c0_endp = C+m_norm*Cstride;
    T c00,c10,c20,c30;
   for (c0=C,a0=A; c0!= c0_endp; c0+=C_sbs_stride,a0+=4) {
      const T* const ap0_endp = a0 + k_norm;
      T* const cp0_endp = c0 + n_norm;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=1,cp0+=1) {
         ap0=a0;
         bp0=b0;
         ZERO4x1(c00,c10,c20,c30);
         for (; ap0!=ap0_endp; ) {
            mul_tmd4x3md3x1_md4x1(c00,c10,c20,c30,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x2) {
            mul_tmd4x2md2x1_md4x1(c00,c10,c20,c30,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmd4x1md1x1_md4x1(c00,c10,c20,c30,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE4x1(c00,c10,c20,c30,cp0,Cstride);
      }
   }
   if (m_marg_el & 0x2) {
      const T* const ap0_endp = a0 + k_norm;
      T* const cp0_endp = c0 + n_norm;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=1,cp0+=1) {
         ap0=a0;
         bp0=b0;
         ZERO2x1(c00,c10);
         for (; ap0!=ap0_endp; ) {
            mul_tmd2x3md3x1_md2x1(c00,c10,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x2) {
            mul_tmd2x2md2x1_md2x1(c00,c10,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmd2x1md1x1_md2x1(c00,c10,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE2x1(c00,c10,cp0,Cstride);
      }
      c0 += Cstride*2;
      a0 += 2;
   }
   if (m_marg_el & 0x1) {
      const T* const ap0_endp = a0 + k_norm;
      T* const cp0_endp = c0 + n_norm;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=1,cp0+=1) {
         ap0=a0;
         bp0=b0;
         ZERO1x1(c00);
         for (; ap0!=ap0_endp; ) {
            mul_tmd1x3md3x1_md1x1(c00,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x2) {
            mul_tmd1x2md2x1_md1x1(c00,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmd1x1md1x1_md1x1(c00,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE1x1(c00,cp0,Cstride);
      }
   }
}

/* Fixed M,K,N = 80,78,20 L0-blocked matrix matrix multiply. */
template<class T>
static void
mdtmd_acbc_md_l1(const T *const A, const T *const B, T *const C, const int Astride, const int Bstride, const int Cstride, const T alpha)
{
   const T *a0,*b0;
   T *c0;
   const T *ap0;
   const T *bp0;
   T *cp0;
   const int C_sbs_stride = Cstride*4;
   const int k_norm = 78*Astride;
   T *const c0_endp = C+80*Cstride;
    T c00,c10,c20,c30;
   for (c0=C,a0=A; c0!= c0_endp; c0+=C_sbs_stride,a0+=4) {
      const T* const ap0_endp = a0 + k_norm;
      T* const cp0_endp = c0 + 20;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=1,cp0+=1) {
         ap0=a0;
         bp0=b0;
         ZERO4x1(c00,c10,c20,c30);
         for (; ap0!=ap0_endp; ) {
            mul_tmd4x3md3x1_md4x1(c00,c10,c20,c30,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE4x1(c00,c10,c20,c30,cp0,Cstride);
      }
   }
}

/* Fixed M,N = 35360,23600, Arbitrary K L1-blocked matrix matrix multiply. */
template<class T>
static void
mdtmd_acbc_md_l2_arb_k(int K, const T *const A, const T *const B, T *const C, const int Astride, const int Bstride, const int Cstride, const T alpha)
{
   /* Code for L1-blocked routine. */
   int m2,k2,n2;
   const T *a2,*b2;
   T *c2;
   const T *ap2,*bp2;
   T *cp2;
   for (m2=0; m2<35360; m2+=80) {
      c2 = C + m2*Cstride;
      a2 = A + m2;
      for (n2=0,b2=B,cp2=c2; n2<23600; n2+=20,b2+=20,cp2+=20) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-78; k2+=78,bp2+=78*Bstride,ap2+=78*Astride) {
            mdtmd_acbc_md_l1(ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
         if (k2 < K) {
            mdtmd_acbc_md_l1_arb_k(K-k2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
      }
   }
}

/* Arbitrary M,K,N L1-blocked matrix matrix multiply. */
template<class T>
static void
mdtmd_acbc_md_l2_arb_all(const int M, const int K, const int N, const T *const A, const T *const B, T *const C, const int Astride, const int Bstride, const int Cstride, const T alpha)
{
   /* Code for L1-blocked routine. */
   int m2,k2,n2;
   const T *a2,*b2;
   T *c2;
   const T *ap2,*bp2;
   T *cp2;
   if (M < 81 && K < 79 && N < 21) {
      mdtmd_acbc_md_l1_arb_all(M,K,N,A,B,C,Astride,Bstride,Cstride,alpha);
      return;
   }
   for (m2=0; m2<=M-80; m2+=80) {
      c2 = C + m2*Cstride;
      a2 = A + m2;
      for (n2=0,b2=B,cp2=c2; n2<=N-20; n2+=20,b2+=20,cp2+=20) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-78; k2+=78,bp2+=78*Bstride,ap2+=78*Astride) {
            mdtmd_acbc_md_l1(ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
         if (k2 < K) {
            mdtmd_acbc_md_l1_arb_k(K-k2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
      }
      if (n2 < N) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-78; k2+=78,bp2+=78*Bstride,ap2+=78*Astride) {
            mdtmd_acbc_md_l1_arb_all(80,78,N-n2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
         if (k2 < K) {
            mdtmd_acbc_md_l1_arb_all(80,K-k2,N-n2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
      }
   }
   if (m2 < M) {
      c2 = C + m2*Cstride;
      a2 = A + m2;
      for (n2=0,b2=B,cp2=c2; n2<=N-20; n2+=20,b2+=20,cp2+=20) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-78; k2+=78,bp2+=78*Bstride,ap2+=78*Astride) {
            mdtmd_acbc_md_l1_arb_all(M-m2,78,20,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
         if (k2 < K) {
            mdtmd_acbc_md_l1_arb_all(M-m2,K-k2,20,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
      }
      if (n2 < N) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-78; k2+=78,bp2+=78*Bstride,ap2+=78*Astride) {
            mdtmd_acbc_md_l1_arb_all(M-m2,78,N-n2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
         if (k2 < K) {
            mdtmd_acbc_md_l1_arb_all(M-m2,K-k2,N-n2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
      }
   }
}

/* Fixed M,K,N = 35360,11466,23600 L1-blocked matrix matrix multiply. */
template<class T>
static void
mdtmd_acbc_md_l2(const T *const A, const T *const B, T *const C, const int Astride, const int Bstride, const int Cstride, const T alpha)
{
   /* Code for L1-blocked routine. */
   int m2,k2,n2;
   const T *a2,*b2;
   T *c2;
   const T *ap2,*bp2;
   T *cp2;
   for (m2=0; m2<35360; m2+=80) {
      c2 = C + m2*Cstride;
      a2 = A + m2;
      for (n2=0,b2=B,cp2=c2; n2<23600; n2+=20,b2+=20,cp2+=20) {
         for (k2=0,bp2=b2,ap2=a2; k2<11466; k2+=78,bp2+=78*Bstride,ap2+=78*Astride) {
            mdtmd_acbc_md_l1(ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
      }
   }
}

template<class T>
void
mdtmd_acbc_md(const int M, const int K, const int N, const T *const A, const T *const B, T *const C, const int Astride, const int Bstride, const int Cstride, const T alpha, const T beta)
{
   /* Code for L2-blocked routine. */
   int m3,k3,n3;
   const T *a3,*b3;
   T *c3;
   const T *ap3,*bp3;
   T *cp3;
   {
      T *cprb,*cpre,*cp,*cpe;
      cpre = C + M*Cstride;
      for (cprb = C; cprb != cpre; cprb += Cstride) {
         cpe = cprb + N;
         for (cp = cprb; cp != cpe; cp++) {
            *cp *= (beta);
         }
      }
   }
   if (alpha == 0.0) {
      return;
   }
   if (M < 35361 && K < 11467 && N < 23601) {
      mdtmd_acbc_md_l2_arb_all(M,K,N,A,B,C,Astride,Bstride,Cstride,alpha);
      return;
   }
   for (m3=0; m3<=M-35360; m3+=35360) {
      c3 = C + m3*Cstride;
      a3 = A + m3;
      for (n3=0,b3=B,cp3=c3; n3<=N-23600; n3+=23600,b3+=23600,cp3+=23600) {
         for (k3=0,bp3=b3,ap3=a3; k3<=K-11466; k3+=11466,bp3+=11466*Bstride,ap3+=11466*Astride) {
            mdtmd_acbc_md_l2(ap3,bp3,cp3,Astride,Bstride,Cstride,alpha);
         }
         if (k3 < K) {
            mdtmd_acbc_md_l2_arb_k(K-k3,ap3,bp3,cp3,Astride,Bstride,Cstride,alpha);
         }
      }
      if (n3 < N) {
         for (k3=0,bp3=b3,ap3=a3; k3<=K-11466; k3+=11466,bp3+=11466*Bstride,ap3+=11466*Astride) {
            mdtmd_acbc_md_l2_arb_all(35360,11466,N-n3,ap3,bp3,cp3,Astride,Bstride,Cstride,alpha);
         }
         if (k3 < K) {
            mdtmd_acbc_md_l2_arb_all(35360,K-k3,N-n3,ap3,bp3,cp3,Astride,Bstride,Cstride,alpha);
         }
      }
   }
   if (m3 < M) {
      c3 = C + m3*Cstride;
      a3 = A + m3;
      for (n3=0,b3=B,cp3=c3; n3<=N-23600; n3+=23600,b3+=23600,cp3+=23600) {
         for (k3=0,bp3=b3,ap3=a3; k3<=K-11466; k3+=11466,bp3+=11466*Bstride,ap3+=11466*Astride) {
            mdtmd_acbc_md_l2_arb_all(M-m3,11466,23600,ap3,bp3,cp3,Astride,Bstride,Cstride,alpha);
         }
         if (k3 < K) {
            mdtmd_acbc_md_l2_arb_all(M-m3,K-k3,23600,ap3,bp3,cp3,Astride,Bstride,Cstride,alpha);
         }
      }
      if (n3 < N) {
         for (k3=0,bp3=b3,ap3=a3; k3<=K-11466; k3+=11466,bp3+=11466*Bstride,ap3+=11466*Astride) {
            mdtmd_acbc_md_l2_arb_all(M-m3,11466,N-n3,ap3,bp3,cp3,Astride,Bstride,Cstride,alpha);
         }
         if (k3 < K) {
            mdtmd_acbc_md_l2_arb_all(M-m3,K-k3,N-n3,ap3,bp3,cp3,Astride,Bstride,Cstride,alpha);
         }
      }
   }
}


END_BL_NAMESPACE


#endif



